{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zuppa R-CNN Stage 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules for MaskRCNN\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import collections\n",
    "from imgaug import augmenters as iaa\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root directory of the project\n",
    "# ROOT_DIR = os.path.abspath(\"C:/mask/Mask_RCNN/\")\n",
    "MASK_RCNN_ROOT_DIR = \"../Mask_RCNN/\"\n",
    "COCO_ROOT_DIR = \".\"\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MASK_RCNN_ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(COCO_ROOT_DIR, \"samples/zuppa/\"))  # To find local version\n",
    "import zuppa as coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(COCO_ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"mask_rcnn_zuppa_0091.h5\")\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0043.h5\") #lowest validation loss\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0100.h5\") #max epoch before augmentation\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0127.h5\") #epoch with augmentation (teh best one)\n",
    "COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0149.h5\") #before extreme augmentation & training all data\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0174.h5\") #extreme augmentation & train all data #likely overfit\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/train\") #test back on images used for training\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/val\") #test on images used for validation (proper)\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images2\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images3\") #other directory for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU used (prints in console/terminal)\n",
    "\n",
    "import tensorflow as tf;\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6695320244017777406\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11152660654049428230\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        10\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                18\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           zuppa\n",
      "NUM_CLASSES                    6\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                400\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.BalloonConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE=0.7\n",
    "    DETECTION_MAX_INSTANCES=10\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/zuppa/anaconda3/envs/opencv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Re-starting from epoch 149\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'Sour', 'Tiger', 'Flower', 'Lychee', 'Milo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def class_id_to_class_name(class_id):\n",
    "    return class_names[class_id]\n",
    "\n",
    "\n",
    "\n",
    "def detect(image):\n",
    "# Run detection\n",
    "    SHOW_VERBOSE = False\n",
    "#     SHOW_VERBOSE = True\n",
    "    results = model.detect([image], verbose=SHOW_VERBOSE)\n",
    "\n",
    "#     print(len(results))\n",
    "\n",
    "    # Visualize results\n",
    "    r = results[0] #so far there's only 1 result\n",
    "\n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'])\n",
    "\n",
    "\n",
    "    detected_class_names = list(map(class_id_to_class_name, r['class_ids']))\n",
    "#     print(detected_class_names)\n",
    "#     print(list(zip(r['class_ids'], r['scores'])))\n",
    "#     print(list(zip(detected_class_names, r['scores'])))\n",
    "\n",
    "    # ref https://stackoverflow.com/a/2162045/3553367\n",
    "    counter=collections.Counter(detected_class_names)\n",
    "    # ref: https://stackoverflow.com/a/17930886/3553367\n",
    "    print(sorted(counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules for Webcam\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Functions for webcam\n",
    "\n",
    "#Use 'jpeg' instead of 'png' (~5 times faster)\n",
    "def showarray(a, prev_display_id=None, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    obj = IPython.display.Image(data=f.getvalue())\n",
    "    if prev_display_id is not None:\n",
    "        IPython.display.update_display(obj, display_id=prev_display_id)\n",
    "        return prev_display_id\n",
    "    else:\n",
    "        return IPython.display.display(obj, display_id=True)\n",
    "    \n",
    "def get_frame(cam):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    #flip image for natural viewing\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "cameras = []\n",
    "\n",
    "def init_cameras():\n",
    "    \n",
    "    for usb_camera in usb_cameras:\n",
    "\n",
    "        cam = cv2.VideoCapture(usb_camera.get('path'))\n",
    "\n",
    "#         cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "#         cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "        cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "        cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "        \n",
    "#         cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "#         cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "#         cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "#         cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "\n",
    "    #     cam.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25) #ref: https://github.com/opencv/opencv/issues/9738#issuecomment-346584044\n",
    "    #     cam.set(cv2.CAP_PROP_EXPOSURE, 0.01)\n",
    "    #     cam.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0)\n",
    "    #     cam.set(cv2.CAP_PROP_EXPOSURE, -4.0)\n",
    "\n",
    "        cameras.append({\n",
    "            \"name\": usb_camera.get('name'),\n",
    "            'cam': cam,\n",
    "            'display_id': None,\n",
    "            \"offset\":  usb_camera.get('offset'),\n",
    "        })\n",
    "        \n",
    "def stop_cameras():      \n",
    "    for camera in cameras:\n",
    "        cam = camera.get('cam')\n",
    "        if(cam):\n",
    "            cam.release()\n",
    "        \n",
    "def test_cams(usb_cameras):\n",
    "    for usb_camera in usb_cameras:\n",
    "#         print(index)\n",
    "        cap = cv2.VideoCapture()\n",
    "        cap.open(usb_camera.get('path'))\n",
    "        if cap.isOpened():\n",
    "            print(\"active: \", usb_camera.get('path'))\n",
    "        else:\n",
    "            print(\"inactive: \", usb_camera.get('path'))\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'path': '/dev/zuppacamera0', 'name': 'zuppacamera0', 'offset': 0.48}, {'path': '/dev/zuppacamera1', 'name': 'zuppacamera1', 'offset': 0.54}]\n",
      "active:  /dev/zuppacamera0\n",
      "active:  /dev/zuppacamera1\n"
     ]
    }
   ],
   "source": [
    "usb_cameras = [\n",
    "    {\n",
    "        \"path\": \"/dev/zuppacamera0\",\n",
    "        \"name\": \"zuppacamera0\",\n",
    "        \"offset\": 0.48,\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"/dev/zuppacamera1\",\n",
    "        \"name\": \"zuppacamera1\",\n",
    "        \"offset\": 0.54,\n",
    "    }\n",
    "]\n",
    "\n",
    "print(usb_cameras)\n",
    "test_cams(usb_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run on images from: captured\n"
     ]
    }
   ],
   "source": [
    "RUN_CAPTURED = True\n",
    "\n",
    "captures_dir = \"captures\" \n",
    "\n",
    "captured_dir = \"captured\" \n",
    "#rename folder to use previously captured images.\n",
    "# Image file names must like:\n",
    "# 2019-11-10_21:24:38_camera_0.jpg\n",
    "# 2019-11-10_21:24:38_camera_2.jpg\n",
    "    \n",
    "if RUN_CAPTURED:\n",
    "    print(\"Will run on images from: \" + captured_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture Images and/or Run Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------DONE------\n"
     ]
    }
   ],
   "source": [
    "### RUN OBJECT DETECTION\n",
    "    \n",
    "    \n",
    "import re\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/train\") #test back on images used for training\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/val\") #test on images used for validation (proper)\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images2\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images3\") #other directory for testing\n",
    "if RUN_CAPTURED:\n",
    "    IMAGE_DIR = os.path.join(COCO_ROOT_DIR, captured_dir) #other directory for testing\n",
    "\n",
    "    # Load a random image from the images folder\n",
    "\n",
    "    file_names = next(os.walk(IMAGE_DIR))[2] #all\n",
    "    file_names = list(filter(lambda x: x.endswith('.jpg'), file_names))\n",
    "\n",
    "    # file_names = [random.choice(file_names)] #random one\n",
    "    # file_names = [file_names[-1]] #last one\n",
    "\n",
    "\n",
    "    # print(file_names)\n",
    "else:\n",
    "    file_names = [] #do not run on any files\n",
    "    \n",
    "    \n",
    "for index, file_name in enumerate(file_names):\n",
    "        \n",
    "    print('------------------------')\n",
    "    print(index + 1, '/' ,len(file_names), 'images')\n",
    "    print(file_name)\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    offset = 0.\n",
    "    file_name_search = re.search('(.*)_camera_(.*)_(.*).jpg', file_name, re.IGNORECASE)\n",
    "\n",
    "    if file_name_search:\n",
    "        camera_name = file_name_search.group(2)\n",
    "            \n",
    "#         print(camera_name)\n",
    "    \n",
    "        #ref: https://stackoverflow.com/a/8653568/3553367\n",
    "        usb_camera =  next(usb_camera for usb_camera in usb_cameras if usb_camera[\"name\"] == camera_name)\n",
    "        offset = usb_camera.get('offset')\n",
    "#     print(offset)\n",
    "        \n",
    "    image_crop = skimage.io.imread(os.path.join(IMAGE_DIR, file_name))\n",
    "\n",
    "    \n",
    "    TEST_AUGMENTATION=True\n",
    "    TEST_AUGMENTATION=False\n",
    "\n",
    "    if(TEST_AUGMENTATION):\n",
    "\n",
    "        augmentation = iaa.Sequential([\n",
    "            # iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "            iaa.Fliplr(0.5),  # horizontally flip 50% of the images\n",
    "        #     iaa.Dropout([0.05, 0.1]),\n",
    "            # blur images with a sigma of 0 to 3.0\n",
    "            iaa.Sometimes(0.5, iaa.Affine(scale=(0.9, 1.1))),\n",
    "            iaa.Sometimes(0.5, iaa.Affine(shear=(-16, 16))),\n",
    "        #     iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "            iaa.Sometimes(0.5, iaa.ContrastNormalization((0.7, 1.3))),\n",
    "            iaa.Sometimes(0.5, iaa.PiecewiseAffine(scale=(0.00, 0.015))),\n",
    "            iaa.Sometimes(0.5, iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25))\n",
    "        ])\n",
    "\n",
    "        image_aug = augmentation.augment_image(image_crop)\n",
    "        skimage.io.imshow(image_aug)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    RUN_DETECTION=True    \n",
    "#         RUN_DETECTION=False\n",
    "\n",
    "    if(RUN_DETECTION):\n",
    "        detect(image_crop)\n",
    "        if(TEST_AUGMENTATION):\n",
    "            detect(image_aug)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(t2 - t1, 's')\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"------DONE------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_cameras()\n",
    "init_cameras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fb5d06973e4dfaa58df913757a6245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Capture All', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ed9f6737a41d6b71f1bc04843dec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "button = widgets.Button(description=\"Capture All\")\n",
    "button_output = widgets.Output()\n",
    "\n",
    "display(button, button_output)\n",
    "\n",
    "final_dir = captures_dir\n",
    "\n",
    "if not os.path.exists(final_dir):\n",
    "    os.makedirs(final_dir)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with button_output:\n",
    "\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        \n",
    "        print(\"------ START CAPTURES ------\")\n",
    "#         print(\"Capturing...\")\n",
    "        \n",
    "        datetime_str = datetime.today().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        capturesss = [];\n",
    "        for camera_num, camera in enumerate(cameras):\n",
    "            name = camera.get('name')\n",
    "            cam = camera.get('cam')\n",
    "            for x in range(10): #lame way to clear image buffer\n",
    "                frame = get_frame(cam)\n",
    "\n",
    "            #crop start\n",
    "            offset = camera.get('offset')\n",
    "            image = frame\n",
    "            width = image.shape[1]\n",
    "            height = image.shape[0]\n",
    "#             print('width', width)\n",
    "#             print('height', height)\n",
    "#             print('offset', offset)\n",
    "\n",
    "            y_half = int((height-1)*offset) #cut of 50% of from top\n",
    "\n",
    "            image_areas = []\n",
    "\n",
    "            # top half\n",
    "            image_areas.append({\n",
    "                \"name\": \"t\",\n",
    "                \"y1\": 0,\n",
    "                \"y2\": y_half,\n",
    "                \"x1\": 0,\n",
    "                \"x2\": width-1,\n",
    "                \"rotate\": True,\n",
    "            })\n",
    "\n",
    "            # bottom half\n",
    "            image_areas.append({\n",
    "                \"name\": \"b\",\n",
    "                \"y1\": y_half,\n",
    "                \"y2\": height,\n",
    "                \"x1\": 0,\n",
    "                \"x2\": width-1,\n",
    "                \"rotate\": False,\n",
    "            })\n",
    "\n",
    "\n",
    "            for image_area in image_areas:\n",
    "\n",
    "                image_crop = image[image_area.get('y1'):image_area.get('y2'), image_area.get('x1'):image_area.get('x2')]\n",
    "\n",
    "                save_image = image_crop\n",
    "                \n",
    "                ENABLE_ROTATE = False\n",
    "                ENABLE_ROTATE = True\n",
    "                \n",
    "                if(ENABLE_ROTATE and image_area.get('rotate') is True):\n",
    "                    #ref: https://www.tutorialkart.com/opencv/python/opencv-python-rotate-image/\n",
    "                    (h, w) = image_crop.shape[:2]\n",
    "                    # calculate the center of the image\n",
    "                    center = (w / 2, h / 2)\n",
    "\n",
    "                    M = cv2.getRotationMatrix2D(center, 180, 1.0)\n",
    "                    save_image = cv2.warpAffine(image_crop, M, (w, h))\n",
    "\n",
    "                final_name = datetime_str+'_camera_'+str(name)+'_'+image_area.get('name')+'.jpg'\n",
    "                cv2.imwrite(final_dir + '/' + final_name, save_image) #need to create folder called captures first!\n",
    "                \n",
    "                capturesss.append({\n",
    "                    \"image\": cv2.cvtColor(save_image, cv2.COLOR_BGR2RGB),\n",
    "                    \"rotate\": image_area.get('rotate')\n",
    "                })\n",
    "                \n",
    "                image_crop = cv2.cvtColor(image_crop, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                showarray(image_crop)\n",
    "                \n",
    "        print(\"------ START DETECTIONS ------\")\n",
    "\n",
    "        for capturess in capturesss:\n",
    "                detect(capturess.get('image'))\n",
    "            \n",
    "        print(\"------DONE------\")\n",
    "#         IPython.display.clear_output(wait=True)\n",
    "        \n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
