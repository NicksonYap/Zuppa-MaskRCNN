{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zuppa R-CNN Stage 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules for MaskRCNN\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import collections\n",
    "from imgaug import augmenters as iaa\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root directory of the project\n",
    "# ROOT_DIR = os.path.abspath(\"C:/mask/Mask_RCNN/\")\n",
    "MASK_RCNN_ROOT_DIR = \"../Mask_RCNN/\"\n",
    "COCO_ROOT_DIR = \".\"\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MASK_RCNN_ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(COCO_ROOT_DIR, \"samples/zuppa/\"))  # To find local version\n",
    "import zuppa as coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(COCO_ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"mask_rcnn_zuppa_0091.h5\")\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0043.h5\") #lowest validation loss\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0100.h5\") #max epoch before augmentation\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0127.h5\") #epoch with augmentation (teh best one)\n",
    "COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0149.h5\") #before extreme augmentation & training all data\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0174.h5\") #extreme augmentation & train all data #likely overfit\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/train\") #test back on images used for training\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/val\") #test on images used for validation (proper)\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images2\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images3\") #other directory for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU used (prints in console/terminal)\n",
    "\n",
    "import tensorflow as tf;\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16188186322262424987\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4246934011108688632\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        10\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                18\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'rpn_bbox_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           zuppa\n",
      "NUM_CLASSES                    6\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                400\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.BalloonConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE=0.7\n",
    "    DETECTION_MAX_INSTANCES=10\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Re-starting from epoch 149\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'Sour', 'Tiger', 'Flower', 'Lychee', 'Milo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def class_id_to_class_name(class_id):\n",
    "    return class_names[class_id]\n",
    "\n",
    "\n",
    "\n",
    "def detect(image):\n",
    "# Run detection\n",
    "    SHOW_VERBOSE = False\n",
    "#     SHOW_VERBOSE = True\n",
    "    results = model.detect([image], verbose=SHOW_VERBOSE)\n",
    "\n",
    "#     print(len(results))\n",
    "\n",
    "    # Visualize results\n",
    "    r = results[0] #so far there's only 1 result\n",
    "\n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'])\n",
    "\n",
    "\n",
    "    detected_class_names = list(map(class_id_to_class_name, r['class_ids']))\n",
    "#     print(detected_class_names)\n",
    "#     print(list(zip(r['class_ids'], r['scores'])))\n",
    "#     print(list(zip(detected_class_names, r['scores'])))\n",
    "\n",
    "    # ref https://stackoverflow.com/a/2162045/3553367\n",
    "    counter=collections.Counter(detected_class_names)\n",
    "    # ref: https://stackoverflow.com/a/17930886/3553367\n",
    "    print(sorted(counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules for Webcam\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "# Functions for webcam\n",
    "\n",
    "#Use 'jpeg' instead of 'png' (~5 times faster)\n",
    "def showarray(a, prev_display_id=None, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    obj = IPython.display.Image(data=f.getvalue())\n",
    "    if prev_display_id is not None:\n",
    "        IPython.display.update_display(obj, display_id=prev_display_id)\n",
    "        return prev_display_id\n",
    "    else:\n",
    "        return IPython.display.display(obj, display_id=True)\n",
    "    \n",
    "def get_frame(cam):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    #flip image for natural viewing\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def get_camera_indexes(max=10):\n",
    "    # ref: https://stackoverflow.com/a/53310665/3553367\n",
    "    arr = []\n",
    "    for index in range(0, max):\n",
    "#         print(index)\n",
    "        cap = cv2.VideoCapture()\n",
    "        cap.open(index)\n",
    "        if cap.isOpened():\n",
    "            arr.append(index)\n",
    "        cap.release()\n",
    "    return arr\n",
    "\n",
    "# List of working cameras\n",
    "camera_indexes = get_camera_indexes()\n",
    "\n",
    "print(camera_indexes)\n",
    "\n",
    "cameras = []\n",
    "\n",
    "def init_cameras():\n",
    "    \n",
    "    for camera_index in camera_indexes:\n",
    "\n",
    "        cam = cv2.VideoCapture(camera_index)\n",
    "\n",
    "\n",
    "        # cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "        # cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "        cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "        cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "    #     cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1024)\n",
    "    #     cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1024\n",
    "\n",
    "    #     cam.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25) #ref: https://github.com/opencv/opencv/issues/9738#issuecomment-346584044\n",
    "    #     cam.set(cv2.CAP_PROP_EXPOSURE, 0.01)\n",
    "    #     cam.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0)\n",
    "    #     cam.set(cv2.CAP_PROP_EXPOSURE, -4.0)\n",
    "\n",
    "        cameras.append({\n",
    "            \"camera_index\": camera_index,\n",
    "            'cam': cam,\n",
    "            'display_id': None,\n",
    "        })\n",
    "        \n",
    "def stop_cameras():      \n",
    "    for camera in cameras:\n",
    "        cam = camera.get('cam')\n",
    "        if cam is not None:\n",
    "            cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will capture new images\n"
     ]
    }
   ],
   "source": [
    "CAPTURE_IMAGES = True\n",
    "# CAPTURE_IMAGES = False # uncomment this to use captured images\n",
    "\n",
    "if not CAPTURE_IMAGES:\n",
    "    use_dir = \"captured/tests\" \n",
    "    #rename folder to use previously captured images.\n",
    "    # Image file names must like:\n",
    "    # 2019-11-10_21:24:38_camera_0.jpg\n",
    "    # 2019-11-10_21:24:38_camera_2.jpg\n",
    "    print(\"Will use images from: \" + use_dir)\n",
    "else:\n",
    "    print(\"Will capture new images\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7df166798f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Convert the image from OpenCV BGR format to matplotlib RGB format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# to display the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdisplay_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.1) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CAPTURE_IMAGES:\n",
    "    stop_cameras()\n",
    "\n",
    "if CAPTURE_IMAGES:\n",
    "    print(\"Capturing...\")\n",
    "\n",
    "    capture_dir = './captures'\n",
    "\n",
    "    init_cameras()\n",
    "    time.sleep(1)\n",
    "\n",
    "    for camera_num, camera in enumerate(cameras):\n",
    "\n",
    "        camera_index = camera.get('camera_index')\n",
    "        cam = camera.get('cam')\n",
    "        frame = get_frame(cam)\n",
    "\n",
    "        final_name = 'camera_'+str(camera_index)+'.jpg'\n",
    "        cv2.imwrite(capture_dir + '/' + final_name, frame) #need to create folder called captures first!\n",
    "\n",
    "\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        display_id = camera.get('display_id')\n",
    "        if display_id is not None:\n",
    "            showarray(frame, display_id)\n",
    "        else:\n",
    "            display_handle = showarray(frame)\n",
    "            camera['display_id'] = display_handle.display_id\n",
    "\n",
    "    print(\"Captured.\")\n",
    "\n",
    "if CAPTURE_IMAGES:\n",
    "    stop_cameras()\n",
    "    \n",
    "    \n",
    "    \n",
    "### RUN OBJECT DETECTION\n",
    "    \n",
    "    \n",
    "import re\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/train\") #test back on images used for training\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/val\") #test on images used for validation (proper)\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images2\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images3\") #other directory for testing\n",
    "if CAPTURE_IMAGES:\n",
    "    IMAGE_DIR = os.path.join(COCO_ROOT_DIR, capture_dir) #other directory for testing\n",
    "else:\n",
    "    IMAGE_DIR = os.path.join(COCO_ROOT_DIR, use_dir) #other directory for testing\n",
    "\n",
    "\n",
    "# Load a random image from the images folder\n",
    "\n",
    "file_names = next(os.walk(IMAGE_DIR))[2] #all\n",
    "file_names = list(filter(lambda x: x.endswith('.jpg'), file_names))\n",
    "\n",
    "# file_names = [random.choice(file_names)] #random one\n",
    "# file_names = [file_names[-1]] #last one\n",
    "\n",
    "\n",
    "# print(file_names)\n",
    "    \n",
    "# camera_offsets = {\n",
    "#     '0': 0.555,\n",
    "#     '2': 0.48\n",
    "# }\n",
    "    \n",
    "camera_offsets = {\n",
    "    '2': 0.555,\n",
    "    '0': 0.48\n",
    "}\n",
    "    \n",
    "TEST_AUGMENTATION=True\n",
    "TEST_AUGMENTATION=False\n",
    "    \n",
    "if(TEST_AUGMENTATION):\n",
    "    augmentation = iaa.Sequential([\n",
    "        # iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "        iaa.Fliplr(0.5),  # horizontally flip 50% of the images\n",
    "    #     iaa.Dropout([0.05, 0.1]),\n",
    "        # blur images with a sigma of 0 to 3.0\n",
    "        iaa.Sometimes(0.5, iaa.Affine(scale=(0.9, 1.1))),\n",
    "        iaa.Sometimes(0.5, iaa.Affine(shear=(-16, 16))),\n",
    "    #     iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.Sometimes(0.5, iaa.ContrastNormalization((0.7, 1.3))),\n",
    "        iaa.Sometimes(0.5, iaa.PiecewiseAffine(scale=(0.00, 0.015))),\n",
    "        iaa.Sometimes(0.5, iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "for index, file_name in enumerate(file_names):\n",
    "        \n",
    "    print('------------------------')\n",
    "    print(index + 1, '/' ,len(file_names), 'cameras')\n",
    "    print(file_name)\n",
    "\n",
    "    offset = 0.5\n",
    "    if CAPTURE_IMAGES:\n",
    "        file_name_search = re.search('camera_(.*).jpg', file_name, re.IGNORECASE)\n",
    "    else:\n",
    "        file_name_search = re.search('(.*)_camera_(.*).jpg', file_name, re.IGNORECASE)\n",
    "\n",
    "    if file_name_search:\n",
    "        if CAPTURE_IMAGES:\n",
    "            camera_index = file_name_search.group(1)\n",
    "        else:\n",
    "            camera_index = file_name_search.group(2)\n",
    "            \n",
    "#         print(camera_index)\n",
    "        offset = camera_offsets.get(camera_index)\n",
    "    \n",
    "#     print(offset)\n",
    "        \n",
    "    image = skimage.io.imread(os.path.join(IMAGE_DIR, file_name))\n",
    "#     skimage.io.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "#     print('width', width)\n",
    "#     print('height', height)\n",
    "\n",
    "    y_half = int((height-1)*offset) #cut of 50% of from top\n",
    "    \n",
    "        \n",
    "    image_areas = []\n",
    "    \n",
    "    # top half\n",
    "    image_areas.append({\n",
    "        \"y1\": 0,\n",
    "        \"y2\": y_half,\n",
    "        \"x1\": 0,\n",
    "        \"x2\": width-1,\n",
    "        \"rotate\": True,\n",
    "    })\n",
    "\n",
    "    # bottom half\n",
    "    image_areas.append({\n",
    "        \"y1\": y_half,\n",
    "        \"y2\": height,\n",
    "        \"x1\": 0,\n",
    "        \"x2\": width-1,\n",
    "        \"rotate\": False,\n",
    "    })\n",
    "    \n",
    "    \n",
    "    \n",
    "    for image_area in image_areas:\n",
    "        print('------')\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "#         print(image_area)\n",
    "    \n",
    "        image_crop = image[image_area.get('y1'):image_area.get('y2'), image_area.get('x1'):image_area.get('x2')]\n",
    "        \n",
    "        if(image_area.get('rotate') is True):\n",
    "#             image_crop = skimage.transform.rotate(image_crop, 180)\n",
    "            \n",
    "            rot_aug = iaa.Sequential([\n",
    "                iaa.Affine(rotate=(180, 180))\n",
    "            ])\n",
    "            image_crop = rot_aug.augment_image(image_crop)\n",
    "            \n",
    "        skimage.io.imshow(image_crop)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        if(TEST_AUGMENTATION):\n",
    "            image_aug = augmentation.augment_image(image_crop)\n",
    "            skimage.io.imshow(image_aug)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        RUN_DETECTION=True    \n",
    "#         RUN_DETECTION=False\n",
    "\n",
    "        if(RUN_DETECTION):\n",
    "            detect(image_crop)\n",
    "            if(TEST_AUGMENTATION):\n",
    "                detect(image_aug)\n",
    "    \n",
    "        t2 = time.time()\n",
    "        print(t2 - t1, 's')\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"------DONE------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
