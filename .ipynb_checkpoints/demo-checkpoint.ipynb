{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zuppa R-CNN Stage 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import collections\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# Root directory of the project\n",
    "# ROOT_DIR = os.path.abspath(\"C:/mask/Mask_RCNN/\")\n",
    "MASK_RCNN_ROOT_DIR = \"../Mask_RCNN/\"\n",
    "COCO_ROOT_DIR = \".\"\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MASK_RCNN_ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(COCO_ROOT_DIR, \"samples/zuppa/\"))  # To find local version\n",
    "import zuppa as coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(COCO_ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"mask_rcnn_zuppa_0091.h5\")\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0043.h5\") #lowest validation loss\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0100.h5\") #max epoch before augmentation\n",
    "COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0127.h5\") #epoch with augmentation (teh best one)\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0149.h5\") #before extreme augmentation & training all data\n",
    "# COCO_MODEL_PATH = os.path.join(COCO_ROOT_DIR, \"logs/zuppa20190305T1602/mask_rcnn_zuppa_0174.h5\") #extreme augmentation & train all data #likely overfit\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/train\") #test back on images used for training\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/val\") #test on images used for validation (proper)\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images2\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images3\") #other directory for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU used (prints in console/terminal)\n",
    "\n",
    "import tensorflow as tf;\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5613480947333856705\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2965456747061117525\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        10\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                18\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'rpn_bbox_loss': 1.0, 'rpn_class_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           zuppa\n",
      "NUM_CLASSES                    6\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                400\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.BalloonConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE=0.7\n",
    "    DETECTION_MAX_INSTANCES=10\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Re-starting from epoch 127\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'Sour', 'Tiger', 'Flower', 'Lychee', 'Milo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ffe4c4507b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_AUGMENTATION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     augmentation = iaa.Sequential([\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0miaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFliplr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# horizontally flip 50% of the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iaa' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/train\") #test back on images used for training\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"samples/zuppa/dataset/val\") #test on images used for validation (proper)\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images\") #other directory for testing\n",
    "# IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images2\") #other directory for testing\n",
    "IMAGE_DIR = os.path.join(COCO_ROOT_DIR, \"images3\") #other directory for testing\n",
    "\n",
    "\n",
    "IMAGE_CROP = False\n",
    "IMAGE_CROP = True\n",
    "\n",
    "# Load a random image from the images folder\n",
    "\n",
    "file_names = next(os.walk(IMAGE_DIR))[2] #all\n",
    "file_names = list(filter(lambda x: x.endswith('.jpg'), file_names))\n",
    "\n",
    "# file_names = [random.choice(file_names)] #random one\n",
    "file_names = [file_names[-1]] #last one\n",
    "\n",
    "\n",
    "# print(file_names)\n",
    "    \n",
    "def class_id_to_class_name(class_id):\n",
    "    return class_names[class_id]\n",
    "\n",
    "    \n",
    "    \n",
    "TEST_AUGMENTATION=True\n",
    "# TEST_AUGMENTATION=False\n",
    "    \n",
    "if(TEST_AUGMENTATION):\n",
    "    augmentation = iaa.Sequential([\n",
    "        # iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "        iaa.Fliplr(0.5),  # horizontally flip 50% of the images\n",
    "    #     iaa.Dropout([0.05, 0.1]),\n",
    "        # blur images with a sigma of 0 to 3.0\n",
    "        iaa.Sometimes(0.5, iaa.Affine(scale=(0.9, 1.1))),\n",
    "        iaa.Sometimes(0.5, iaa.Affine(shear=(-16, 16))),\n",
    "    #     iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.Sometimes(0.5, iaa.ContrastNormalization((0.7, 1.3))),\n",
    "        iaa.Sometimes(0.5, iaa.PiecewiseAffine(scale=(0.00, 0.015))),\n",
    "        iaa.Sometimes(0.5, iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def detect(image):\n",
    "# Run detection\n",
    "    SHOW_VERBOSE = False\n",
    "#     SHOW_VERBOSE = True\n",
    "    results = model.detect([image], verbose=SHOW_VERBOSE)\n",
    "\n",
    "#     print(len(results))\n",
    "\n",
    "    # Visualize results\n",
    "    r = results[0] #so far there's only 1 result\n",
    "\n",
    "    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'])\n",
    "\n",
    "\n",
    "    detected_class_names = list(map(class_id_to_class_name, r['class_ids']))\n",
    "#     print(detected_class_names)\n",
    "#     print(list(zip(r['class_ids'], r['scores'])))\n",
    "#     print(list(zip(detected_class_names, r['scores'])))\n",
    "\n",
    "    # ref https://stackoverflow.com/a/2162045/3553367\n",
    "    counter=collections.Counter(detected_class_names)\n",
    "    # ref: https://stackoverflow.com/a/17930886/3553367\n",
    "    print(sorted(counter.items()))\n",
    "    \n",
    "    \n",
    "for index, file_name in enumerate(file_names):\n",
    "    print(index + 1, '/' ,len(file_names))\n",
    "    print(file_name)\n",
    "    image = skimage.io.imread(os.path.join(IMAGE_DIR, file_name))\n",
    "    skimage.io.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    if(IMAGE_CROP):\n",
    "        width = image.shape[1]\n",
    "        height = image.shape[0]\n",
    "        print('width', width)\n",
    "        print('height', height)\n",
    "\n",
    "        x1 = 0\n",
    "        x2 = width-1\n",
    "        y1 = int((height-1)*0.42) #cut of 30% of from top\n",
    "        y2 = height\n",
    "        image_crop = image[y1:y2, x1:x2]\n",
    "        skimage.io.imshow(image_crop)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        image = image_crop\n",
    "\n",
    "    \n",
    "    if(TEST_AUGMENTATION):\n",
    "        image_aug = augmentation.augment_image(image)\n",
    "        skimage.io.imshow(image_aug)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    RUN_DETECTION=True    \n",
    "#     RUN_DETECTION=False\n",
    "    \n",
    "    if(RUN_DETECTION):\n",
    "        detect(image)\n",
    "        if(TEST_AUGMENTATION):\n",
    "            detect(image_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
